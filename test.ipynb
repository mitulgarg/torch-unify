{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils._pytree import tree_map"
      ],
      "metadata": {
        "id": "FObYnHxPYnax"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "eSzxF1A1WjZ6"
      },
      "outputs": [],
      "source": [
        "class ManagedTensor:\n",
        "    def __init__(self, tensor: torch.Tensor, device: str = None):\n",
        "        \"\"\"\n",
        "        Initializes the ManagedTensor.\n",
        "\n",
        "        If a specific `device` is not provided, it will automatically move the\n",
        "        tensor to 'cuda' if a GPU is available, otherwise leaving it on the CPU.\n",
        "        \"\"\"\n",
        "        # --- NEW: Smart Constructor Logic ---\n",
        "        if device is None:\n",
        "            # Automatic placement: Use GPU if available\n",
        "            if torch.cuda.is_available():\n",
        "                target_device = torch.device('cuda')\n",
        "                self.tensor = tensor.to(target_device)\n",
        "                self.device = target_device\n",
        "            else:\n",
        "                # Fallback to CPU\n",
        "                self.tensor = tensor\n",
        "                self.device = tensor.device\n",
        "        else:\n",
        "            # Manual override: Respect the user's choice\n",
        "            target_device = torch.device(device)\n",
        "            self.tensor = tensor.to(target_device)\n",
        "            self.device = target_device\n",
        "\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Managed({self.tensor.shape}, device='{self.device}')\"\n",
        "\n",
        "    @property\n",
        "    def shape(self):\n",
        "        return self.tensor.shape\n",
        "\n",
        "    @property\n",
        "    def dtype(self):\n",
        "        return self.tensor.dtype\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.tensor, name)\n",
        "\n",
        "    def sum(self, *args, **kwargs):\n",
        "        return torch.sum(self, *args, **kwargs)\n",
        "\n",
        "    def relu(self, *args, **kwargs):\n",
        "        return F.relu(self, *args, **kwargs)\n",
        "\n",
        "    def __add__(self, other):\n",
        "        return torch.add(self, other)\n",
        "\n",
        "    def __mul__(self, other):\n",
        "        return torch.mul(self, other)\n",
        "\n",
        "    # This handles cases like `some_number * managed_tensor`\n",
        "    def __rmul__(self, other):\n",
        "        return torch.mul(other, self)\n",
        "\n",
        "    @classmethod\n",
        "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
        "        if kwargs is None:\n",
        "            kwargs = {}\n",
        "\n",
        "        # --- NEW: Simplified Device Selection Rule ---\n",
        "        target_device = torch.device('cpu')\n",
        "\n",
        "        # Find if any tensor is on a GPU. If so, the target is 'cuda'.\n",
        "        flat_args, _ = torch.utils._pytree.tree_flatten(list(args) + list(kwargs.values()))\n",
        "        for arg in flat_args:\n",
        "            if isinstance(arg, ManagedTensor) and arg.device.type == 'cuda':\n",
        "                target_device = torch.device('cuda')\n",
        "                break # Found a GPU tensor, no need to look further\n",
        "\n",
        "        def move_and_unwrap(x):\n",
        "            if isinstance(x, ManagedTensor):\n",
        "                if x.device != target_device:\n",
        "                    x.tensor, x.device = x.tensor.to(target_device), target_device\n",
        "                return x.tensor\n",
        "            if isinstance(x, torch.Tensor):\n",
        "                return x.to(target_device)\n",
        "            return x\n",
        "\n",
        "        new_args = tree_map(move_and_unwrap, args)\n",
        "        new_kwargs = tree_map(move_and_unwrap, kwargs)\n",
        "        raw_output = func(*new_args, **new_kwargs)\n",
        "\n",
        "        def wrap_output(x):\n",
        "            if isinstance(x, torch.Tensor): return ManagedTensor(x)\n",
        "            return x\n",
        "\n",
        "        return tree_map(wrap_output, raw_output)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------------\n",
        "# 2. Inference Checker\n",
        "# -------------------------------------------------------------------\n",
        "print(\"--- Running Verification Checks ---\")\n",
        "\n",
        "# Check 1: Automatic GPU placement\n",
        "print(\"\\n[Check 1] Does it auto-detect the GPU?\")\n",
        "# We create the tensor without specifying a device.\n",
        "# The constructor should automatically move it to 'cuda'.\n",
        "auto_gpu_tensor = ManagedTensor(torch.randn(2, 2))\n",
        "print(f\"Tensor was automatically placed on: {auto_gpu_tensor.device}\")\n",
        "assert auto_gpu_tensor.device.type == 'cuda'\n",
        "print(\"✅ Passed!\")\n",
        "\n",
        "# Check 2: Manual CPU override\n",
        "print(\"\\n[Check 2] Can we force placement on CPU?\")\n",
        "# We explicitly ask for the CPU, even though a GPU is available.\n",
        "manual_cpu_tensor = ManagedTensor(torch.randn(2, 2), device='cpu')\n",
        "print(f\"Tensor was manually placed on: {manual_cpu_tensor.device}\")\n",
        "assert manual_cpu_tensor.device.type == 'cpu'\n",
        "print(\"✅ Passed!\")\n",
        "\n",
        "# Check 3: Automatic device resolution during an operation\n",
        "print(\"\\n[Check 3] Does it auto-move tensors during an operation?\")\n",
        "print(f\"Before op: manual_cpu_tensor is on {manual_cpu_tensor.device}\")\n",
        "print(f\"Before op: auto_gpu_tensor is on {auto_gpu_tensor.device}\")\n",
        "\n",
        "# The operation should run on the GPU. The class should move the CPU tensor.\n",
        "result = manual_cpu_tensor + auto_gpu_tensor\n",
        "\n",
        "print(f\"After op: manual_cpu_tensor moved to {manual_cpu_tensor.device}\")\n",
        "print(f\"After op: result tensor is on {result.device}\")\n",
        "assert manual_cpu_tensor.device.type == 'cuda'\n",
        "assert result.device.type == 'cuda'\n",
        "print(\"✅ Passed!\")\n",
        "\n",
        "\n",
        "# Check 4: Method call syntax\n",
        "print(\"\\n[Check 4] Do method calls like .sum() work?\")\n",
        "total = result.sum()\n",
        "print(f\"Result of .sum() is {total.tensor} on device {total.device}\")\n",
        "assert total.device.type == 'cuda'\n",
        "print(\"✅ Passed!\")\n",
        "\n",
        "print(\"\\n--- All checks passed successfully! ---\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi8tLWq-W4iH",
        "outputId": "30d59aae-229a-4228-9470-34fe8bcd241f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Verification Checks ---\n",
            "\n",
            "[Check 1] Does it auto-detect the GPU?\n",
            "Tensor was automatically placed on: cuda\n",
            "✅ Passed!\n",
            "\n",
            "[Check 2] Can we force placement on CPU?\n",
            "Tensor was manually placed on: cpu\n",
            "✅ Passed!\n",
            "\n",
            "[Check 3] Does it auto-move tensors during an operation?\n",
            "Before op: manual_cpu_tensor is on cpu\n",
            "Before op: auto_gpu_tensor is on cuda\n",
            "After op: manual_cpu_tensor moved to cuda\n",
            "After op: result tensor is on cuda\n",
            "✅ Passed!\n",
            "\n",
            "[Check 4] Do method calls like .sum() work?\n",
            "Result of .sum() is 6.897651672363281 on device cuda\n",
            "✅ Passed!\n",
            "\n",
            "--- All checks passed successfully! ---\n"
          ]
        }
      ]
    }
  ]
}